{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8435688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/Julia/doc/cscs_gpu_course/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.DEFAULT_IO[] = stdout  # Julia 1.6.1 bug (Pkg.jl#2542)\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a459ca",
   "metadata": {},
   "source": [
    "# Programming models\n",
    "\n",
    "There are different ways of programming (NVIDIA) GPUs in Julia, at different levels of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e37034",
   "metadata": {},
   "source": [
    "## Array programming\n",
    "\n",
    "The easiest way to use a GPU is via vectorized array operations. Each of these operations will be backed by one or more GPU kernels, either natively written in Julia or from some application library. As long as your data is large enough you'll should be able to get some nice speed-ups.\n",
    "\n",
    "For NVIDIA GPUs, you use the `CuArray` type from CUDA.jl, which serves a dual porpose:\n",
    "- a managed container for GPU memory\n",
    "- a way to dispatch to operations that execute on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae46b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0  2.0\n",
       " 3.0  4.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "A = CuArray([1. 2.; 3. 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6cdef",
   "metadata": {},
   "source": [
    "Memory management will be discussed in detail in a next notebook, but for now it's enough to remember that a CuArray is **a CPU object representing memory on the GPU**. It will be automatically freed when all references have been removed, and the garbage collector runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739475e",
   "metadata": {},
   "source": [
    "The goal of `CuArray` is to make it easy to program GPUs using array operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1fd7735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
       "  7.0  10.0\n",
       " 15.0  22.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will automatically use CUBLAS\n",
    "A * A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f8928fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0   4.0\n",
       " 9.0  16.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whereas this operation will use a native broadcast kernel\n",
    "A .* A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f0322",
   "metadata": {},
   "source": [
    "This works by specializing certain methods with a GPU-specialized implementation, either for:\n",
    "- compatibility: not all CPU implementations work on the GPU\n",
    "- performance: GPUs have a different programming model so might require optimized implementations\n",
    "\n",
    "This generally works pretty well, the goal is to get as close to the CPU `Array` type's functionality as possible, and entire applications have been built on top of CuArray's array functionality. So instead let's highlight what can go wrong if you don't call into a GPU-specialized implementation where you need one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683bfb4",
   "metadata": {},
   "source": [
    "### Compatibility: Calling into C libraries\n",
    "\n",
    "A common issue arises when calling CPU-specific code, e.g. in some C library, using a GPU array. This generally does not work, because GPU pointers are not dereferencable on the CPU. To prevent this from crashing, we introduce a GPU-specific pointer type and disallow conversions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c6005b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: cannot take the CPU address of a CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}",
     "output_type": "error",
     "traceback": [
      "ArgumentError: cannot take the CPU address of a CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}",
      "",
      "Stacktrace:",
      " [1] unsafe_convert(#unused#::Type{Ptr{Int64}}, x::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer})",
      "   @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/array.jl:315",
      " [2] unsafe_convert(#unused#::Type{Ptr{Float32}}, a::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer})",
      "   @ Base ./pointer.jl:66",
      " [3] top-level scope",
      "   @ ./In[7]:1",
      " [4] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "ccall(:whatever, Nothing, (Ptr{Float32},), A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c3981",
   "metadata": {},
   "source": [
    "In that case, either you need to use different (supported) array operations, or fix the implementation in CUDA.jl. Such a fix can mean using functions from a CUDA library, using existing operations, or writing your own kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010f0f3",
   "metadata": {},
   "source": [
    "### Performance: Scalar iteration\n",
    "\n",
    "A key performance issue comes from the fact that a `CuArray` instance is a CPU object representing a chunk of memory on the GPU. That means we invoke the GPU for every CPU operation invoked on a CuArray. That is OK for scalar invocations, where the GPU operation will have to do a bunch of work, but is very bad when you have CPU code performing a bunch of small scalar operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba2b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar indexing on task Task (runnable) @0x00007fbfcf45ee00.\n",
      "│ Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "│ This is typically caused by calling an iterating implementation of a method.\n",
      "│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "│ and therefore are only permitted from the REPL for prototyping purposes.\n",
      "│ If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "└ @ GPUArrays /home/tim/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CuArray(1:10)\n",
    "A_sum = zero(eltype(A))\n",
    "for I in eachindex(A)\n",
    "    A_sum += A[I]\n",
    "end\n",
    "A_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee038f2c",
   "metadata": {},
   "source": [
    "Because of this kind of programming pattern, iterating the array and fetching one scalar at a time (hence 'scalar iteration'), being so slow CUDA.jl warns about it. With the above snippet, the situation is actually even worse: Not only does every iteration require a GPU operation to fetch an element, the `getindex` call is also the only array operation meaning that the actual summation won't even run on the GPU!\n",
    "\n",
    "The solution here is to use the `sum` function that performs the entire operation as a single step. More on these operations later.\n",
    "To disallow scalar iteration, use the `allowscalar` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303d3f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:33",
      " [2] assertscalar(op::String)",
      "   @ GPUArrays ~/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:53",
      " [3] getindex(xs::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}, I::Int64)",
      "   @ GPUArrays ~/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:86",
      " [4] top-level scope",
      "   @ In[4]:2",
      " [5] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [6] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "CUDA.allowscalar(false)\n",
    "A[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7532d",
   "metadata": {},
   "source": [
    "You should generally always enable this option! It's not by default in interactive sessions because it simplifies porting CPU code, and it's easy to trigger scalar iteration from non performance-sensitive paths (e.g. display methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43200f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 adjoint(::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}) with eltype Int64:\n",
       " 1  2  3  4  5  6  7  8  9  10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b17085c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:33",
      "  [2] assertscalar(op::String)",
      "    @ GPUArrays ~/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:53",
      "  [3] getindex",
      "    @ ~/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:86 [inlined]",
      "  [4] getindex",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/LinearAlgebra/src/adjtrans.jl:202 [inlined]",
      "  [5] _getindex",
      "    @ ./abstractarray.jl:1197 [inlined]",
      "  [6] getindex",
      "    @ ./abstractarray.jl:1170 [inlined]",
      "  [7] getindex",
      "    @ ./subarray.jl:276 [inlined]",
      "  [8] isassigned(::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true}, ::Int64, ::Int64)",
      "    @ Base ./abstractarray.jl:513",
      "  [9] alignment(io::IOContext{IOBuffer}, X::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true}, rows::UnitRange{Int64}, cols::UnitRange{Int64}, cols_if_complete::Int64, cols_otherwise::Int64, sep::Int64)",
      "    @ Base ./arrayshow.jl:67",
      " [10] _print_matrix(io::IOContext{IOBuffer}, X::AbstractVecOrMat{T} where T, pre::String, sep::String, post::String, hdots::String, vdots::String, ddots::String, hmod::Int64, vmod::Int64, rowsA::UnitRange{Int64}, colsA::UnitRange{Int64})",
      "    @ Base ./arrayshow.jl:201",
      " [11] #invokelatest#2",
      "    @ ./essentials.jl:708 [inlined]",
      " [12] invokelatest",
      "    @ ./essentials.jl:706 [inlined]",
      " [13] print_matrix (repeats 2 times)",
      "    @ ./arrayshow.jl:170 [inlined]",
      " [14] print_array",
      "    @ ./arrayshow.jl:327 [inlined]",
      " [15] show(io::IOContext{IOBuffer}, #unused#::MIME{Symbol(\"text/plain\")}, X::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})",
      "    @ Base ./arrayshow.jl:368",
      " [16] limitstringmime(mime::MIME{Symbol(\"text/plain\")}, x::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})",
      "    @ IJulia ~/Julia/depot/packages/IJulia/e8kqU/src/inline.jl:43",
      " [17] display_mimestring",
      "    @ ~/Julia/depot/packages/IJulia/e8kqU/src/display.jl:71 [inlined]",
      " [18] display_dict(x::SubArray{Int64, 2, LinearAlgebra.Adjoint{Int64, CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}}, Tuple{Base.Slice{Base.OneTo{Int64}}, Base.Slice{Base.OneTo{Int64}}}, true})",
      "    @ IJulia ~/Julia/depot/packages/IJulia/e8kqU/src/display.jl:102",
      " [19] #invokelatest#2",
      "    @ ./essentials.jl:708 [inlined]",
      " [20] invokelatest",
      "    @ ./essentials.jl:706 [inlined]",
      " [21] execute_request(socket::ZMQ.Socket, msg::IJulia.Msg)",
      "    @ IJulia ~/Julia/depot/packages/IJulia/e8kqU/src/execute_request.jl:112",
      " [22] #invokelatest#2",
      "    @ ./essentials.jl:708 [inlined]",
      " [23] invokelatest",
      "    @ ./essentials.jl:706 [inlined]",
      " [24] eventloop(socket::ZMQ.Socket)",
      "    @ IJulia ~/Julia/depot/packages/IJulia/e8kqU/src/eventloop.jl:8",
      " [25] (::IJulia.var\"#15#18\")()",
      "    @ IJulia ./task.jl:411"
     ]
    }
   ],
   "source": [
    "view(A', :, :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9398ad",
   "metadata": {},
   "source": [
    "Because of how Julia's type system works, it's easy to trigger non GPU-specialized methods when using array wrappers. Still, for non-interactive code it's recommended to always disable scalar iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c612d",
   "metadata": {},
   "source": [
    "### CuArray isn't device-compatible\n",
    "\n",
    "A more subtle result of `CuArray` being the CPU-side object is that these objects cannot be used directly on the GPU. Instead, a conversion to `CuDeviceArray` happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd79553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTX CompilerJob of kernel #2(CuDeviceVector{Int64, 1}) for sm_75\n",
      "\n",
      "Variables\n",
      "  #self#\u001b[36m::Core.Const(var\"#2#3\"())\u001b[39m\n",
      "  A\u001b[36m::CuDeviceVector{Int64, 1}\u001b[39m\n",
      "\n",
      "Body\u001b[36m::Nothing\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m     return Main.nothing\n"
     ]
    }
   ],
   "source": [
    "@device_code_warntype @cuda (A->nothing)(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8fd2d",
   "metadata": {},
   "source": [
    "Typically, this conversion is hidden and shouldn't affect you as an end user. The only time you need to take care, is when embedding `CuArray`s in a structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c30c38c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "GPU compilation of kernel #4(MyStruct) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type MyStruct, which is not isbits:\n  .inner is of type CuArray which is not isbits.\n    .storage is of type Union{Nothing, CUDA.ArrayStorage{B}} where B which is not isbits.\n    .dims is of type Tuple{Vararg{Int64, N}} where N which is not isbits.\n\n",
     "output_type": "error",
     "traceback": [
      "GPU compilation of kernel #4(MyStruct) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type MyStruct, which is not isbits:\n  .inner is of type CuArray which is not isbits.\n    .storage is of type Union{Nothing, CUDA.ArrayStorage{B}} where B which is not isbits.\n    .dims is of type Tuple{Vararg{Int64, N}} where N which is not isbits.\n\n",
      "",
      "Stacktrace:",
      "  [1] check_invocation(job::GPUCompiler.CompilerJob)",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/validation.jl:66",
      "  [2] macro expansion",
      "    @ ~/Julia/depot/packages/GPUCompiler/AJD5L/src/driver.jl:332 [inlined]",
      "  [3] macro expansion",
      "    @ ~/Julia/depot/packages/TimerOutputs/SSeq1/src/TimerOutput.jl:252 [inlined]",
      "  [4] macro expansion",
      "    @ ~/Julia/depot/packages/GPUCompiler/AJD5L/src/driver.jl:331 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/utils.jl:62",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:326",
      "  [7] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/cache.jl:89",
      "  [8] cufunction(f::var\"#4#5\", tt::Type{Tuple{MyStruct}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:297",
      "  [9] cufunction(f::var\"#4#5\", tt::Type{Tuple{MyStruct}})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:291",
      " [10] top-level scope",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:102",
      " [11] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [12] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "struct MyStruct\n",
    "    inner::CuArray\n",
    "end\n",
    "B = MyStruct(A)\n",
    "@cuda (A->nothing)(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f4aaa",
   "metadata": {},
   "source": [
    "Here, CUDA.jl makes it clear that a `CuArray` isn't GPU compatible because it's not an `isbits` type. The underlying reason is that the automatic conversion from `CuArray` to `CuDeviceArray` doesn't know about your `MyStruct` and how to convert it to something GPU-compatible. This conversion is done using Adapt.jl, and to make this code work you need to teach Adapt about how to convert `MyStruct` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccabb456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTX CompilerJob of kernel #6(MyParametricStruct{CuDeviceVector{Int64, 1}}) for sm_75\n",
      "\n",
      "Variables\n",
      "  #self#\u001b[36m::Core.Const(var\"#6#7\"())\u001b[39m\n",
      "  A\u001b[36m::MyParametricStruct{CuDeviceVector{Int64, 1}}\u001b[39m\n",
      "\n",
      "Body\u001b[36m::Nothing\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m     return Main.nothing\n"
     ]
    }
   ],
   "source": [
    "# to store both a CuArray and a CuDeviceArray\n",
    "# our struct needs to be parametric\n",
    "struct MyParametricStruct{T<:AbstractArray}\n",
    "    inner::T\n",
    "end\n",
    "\n",
    "using Adapt\n",
    "Adapt.adapt_structure(to, x::MyParametricStruct) = MyParametricStruct(adapt(to, x.inner))\n",
    "\n",
    "C = MyParametricStruct(A)\n",
    "@device_code_warntype @cuda (A->nothing)(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ee738d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4083871458683567"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(1024, 1024)\n",
    "B = rand(1024, 1024)\n",
    "sqrt(sum((A-B).^2) / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecb474",
   "metadata": {},
   "source": [
    "## Kernel programming\n",
    "\n",
    "Kernels are scalar functions that get executed multiple times in parallel. Each 'thread' runs on one of the many streaming multiprocessors a GPU has, and threads running on a single SM are called a 'block'. Within a SM, some threads are always executed together; these form a 'warp' of 32 threads. Efficient communication between these entities is required to effectively use the GPU:\n",
    "\n",
    "- between blocks: global memory\n",
    "- within a block: shared memory\n",
    "- within a warp: via registers (shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e4c71",
   "metadata": {},
   "source": [
    "### Hardware indices\n",
    "\n",
    "You can fetch the thread, block and warp index using specific functions that query hardware indices:\n",
    "\n",
    "- `threadIdx()` and `blockDim()`: 3D\n",
    "- `blockIdx()` and `gridDim()`: 3D\n",
    "- `laneid()` and `warpsize()`\n",
    "\n",
    "When you don't need to care about which block a thread is part of, a very common index calculation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf386c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(kernel), Tuple{}}(kernel, CuContext(0x00000000062264a0, instance f02fca961a36c4ad), CuModule(Ptr{Nothing} @0x0000000009d56840, CuContext(0x00000000062264a0, instance f02fca961a36c4ad)), CuFunction(Ptr{Nothing} @0x00000000094ce9c0, CuModule(Ptr{Nothing} @0x0000000009d56840, CuContext(0x00000000062264a0, instance f02fca961a36c4ad))), CUDA.KernelState(Ptr{Nothing} @0x00007fc560c00000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1\n",
      "i = 2\n",
      "i = 3\n",
      "i = 4\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @cushow i\n",
    "    return\n",
    "end\n",
    "@cuda threads=2 blocks=2 kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4247070",
   "metadata": {},
   "source": [
    "### Synchronization\n",
    "\n",
    "If threads are working together -- say, they are using the same global memory, or are communicating using shared memory or finer-grained intrinsics -- you may need to have threads wait on each other. Note that this is only possible **within a block**; different blocks generally cannot wait on one another.\n",
    "\n",
    "Let's look at a contrived example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ff4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 42.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.zeros(512)\n",
    "\n",
    "function kernel(A)\n",
    "    # simple kernel without multiple blocks\n",
    "    i = threadIdx().x\n",
    "    \n",
    "    # first thread sets up the data\n",
    "    if i == 1\n",
    "        A[1] = 42\n",
    "    end\n",
    "    \n",
    "    sync_threads()\n",
    "    \n",
    "    # other threads can now read this data\n",
    "    if i != 1\n",
    "        A[i] = A[1]\n",
    "    end\n",
    "    \n",
    "    return\n",
    "end\n",
    "@cuda threads=length(A) kernel(A)\n",
    "unique(Array(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f23cc",
   "metadata": {},
   "source": [
    "Note how we didn't put `sync_threads()` inside of the branch; All threads need to reach the synchronization point for the kernel to make progress. This makes it dangerous to synchronize from a branch, as the branch cannot be divergent within a block or the kernel would deadlock!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f21b3",
   "metadata": {},
   "source": [
    "When coordinating within the warp, you may need the `sync_warp()` function. A detailed explanation of warp-level programming is out of scope for this notebook, refer to the [NVIDIA developer blog](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade663de",
   "metadata": {},
   "source": [
    "### Atomic operations\n",
    "\n",
    "When you want to use the same global memory from different threads, you may want to use atomic operations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9d0979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.92522f0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum = CUDA.zeros(1)\n",
    "A = CUDA.rand(512)\n",
    "\n",
    "function kernel(A, A_sum)\n",
    "    i = threadIdx().x\n",
    "    CUDA.@atomic A_sum[] += A[i]\n",
    "    return\n",
    "end\n",
    "@cuda threads=length(A) kernel(A, A_sum)\n",
    "Array(A_sum)[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3e656",
   "metadata": {},
   "source": [
    "You shouldn't overuse atomics though, as they generally serialize execution and thus are very expensive! But they may be useful for an initial implementation (i.e. before considering more fine-grained communication), or to reduce values from different blocks (because of the difficulty of synchronizing the grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5e5b7",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "To help with implementing a kernel, there's a couple of helpful macros to generate output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f880a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function kernel()\n",
    "    i = threadIdx().x\n",
    "    @cuprintf \"I'm thread %ld\\n\" Int(i)\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cfed74",
   "metadata": {},
   "source": [
    "However, `@cuprintf` is a bit cumbersome, so we have `@cuprintln` trying to automatically generate an appropriate formatting string, while even supporting string interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0804fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm thread 1\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = threadIdx().x\n",
    "    @cuprintln \"I'm thread $i\"\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6add9d3",
   "metadata": {},
   "source": [
    "And for quick debugging, we have a helpful `@cushow` you can surround expressions with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110e15db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm thread 1\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = @cushow(threadIdx().x)\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c166fc3e",
   "metadata": {},
   "source": [
    "## Exercise: Matrix RMSE\n",
    "\n",
    "As an trivial exercise, try to compute the RMSE of two matrices on the GPU using both array operations and using a GPU kernel (a single kernel, if possible):\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc563644",
   "metadata": {},
   "source": [
    "For prototyping, you can develop the array operation on the CPU (one of the advantages of using array operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa8342d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42389724f0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.rand(10,10)\n",
    "B = CUDA.rand(10,10)\n",
    "sqrt(sum((A-B).^2)/length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50845c3",
   "metadata": {},
   "source": [
    "To 'port' this to the GPU, just change the type of the input arrays to `CuArray` and the computation of C just works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1defb88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4083871458683567"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CuArray(A)\n",
    "B = CuArray(B)\n",
    "sqrt(sum((A-B).^2) / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8d1cf",
   "metadata": {},
   "source": [
    "Now for a CUDA.jl kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e2abb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.42389724"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B)  \n",
    "    i = threadIdx().x\n",
    "\n",
    "    # initialize the memory\n",
    "    if i == 1\n",
    "        C[] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # process an element on each thread\n",
    "    a = A[i]\n",
    "    b = B[i]\n",
    "    CUDA.@atomic C[] += (a-b)^2\n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if i == 1\n",
    "        C[1] = sqrt(C[] / length(A))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=length(A) rmse_kernel(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0450ab",
   "metadata": {},
   "source": [
    "This kernel only works when the array fits in a single block though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "273206e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
     "output_type": "error",
     "traceback": [
      "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
      "",
      "Stacktrace:",
      "  [1] throw_api_error(res::CUDA.cudaError_enum)",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/error.jl:91",
      "  [2] macro expansion",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/error.jl:101 [inlined]",
      "  [3] cuLaunchKernel(f::CuFunction, gridDimX::UInt32, gridDimY::UInt32, gridDimZ::UInt32, blockDimX::UInt32, blockDimY::UInt32, blockDimZ::UInt32, sharedMemBytes::Int64, hStream::CuStream, kernelParams::Vector{Ptr{Nothing}}, extra::Ptr{Nothing})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/lib/utils/call.jl:26",
      "  [4] #27",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:69 [inlined]",
      "  [5] macro expansion",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:33 [inlined]",
      "  [6] macro expansion",
      "    @ ./none:0 [inlined]",
      "  [7] pack_arguments(::CUDA.var\"#27#28\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3}, ::CUDA.KernelState, ::CuDeviceVector{Float32, 1}, ::CuDeviceMatrix{Float32, 1}, ::CuDeviceMatrix{Float32, 1})",
      "    @ CUDA ./none:0",
      "  [8] #launch#26",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:62 [inlined]",
      "  [9] #32",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:136 [inlined]",
      " [10] macro expansion",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:95 [inlined]",
      " [11] macro expansion",
      "    @ ./none:0 [inlined]",
      " [12] convert_arguments",
      "    @ ./none:0 [inlined]",
      " [13] #cudacall#31",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:135 [inlined]",
      " [14] macro expansion",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:204 [inlined]",
      " [15] macro expansion",
      "    @ ./none:0 [inlined]",
      " [16] call(::CUDA.HostKernel{typeof(rmse_kernel), Tuple{CuDeviceVector{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}}, ::CuDeviceVector{Float32, 1}, ::CuDeviceMatrix{Float32, 1}, ::CuDeviceMatrix{Float32, 1}; call_kwargs::Base.Iterators.Pairs{Symbol, Int64, Tuple{Symbol, Symbol}, NamedTuple{(:threads, :blocks), Tuple{Int64, Int64}}})",
      "    @ CUDA ./none:0",
      " [17] (::CUDA.HostKernel{typeof(rmse_kernel), Tuple{CuDeviceVector{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}})(::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, ::Vararg{Any, N} where N; threads::Int64, blocks::Int64, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:462",
      " [18] top-level scope",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:104",
      " [19] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [20] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "A = CUDA.rand(1024,1024)\n",
    "B = CUDA.rand(1024,1024)\n",
    "@cuda threads=length(A) rmse_kernel(C, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a151df",
   "metadata": {},
   "source": [
    "We could just use multiple blocks, since our currently implementation doesn't use any communication between threads. However, in a future notebook we _will_ add such communication, so for now let's make it so that we only need a single block to process this matrix.\n",
    "\n",
    "A good way to do so is to introduce a grid-stride loop. This has been explained in a previous notebook, so try adapting the kernel implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "addd90a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.40774998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B)  \n",
    "    # initialize the memory\n",
    "    if threadIdx().x == 1\n",
    "        C[1] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # grid-stride loop to process each batch in a block\n",
    "    for i in threadIdx().x:blockDim().x:length(A)\n",
    "        a = A[i]\n",
    "        b = B[i]\n",
    "        CUDA.@atomic C[1] += (a-b)^2\n",
    "    end    \n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if threadIdx().x == 1\n",
    "        C[1] = sqrt(C[1] / length(A))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=256 rmse_kernel(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e4dba",
   "metadata": {},
   "source": [
    "## High-level kernel programming\n",
    "\n",
    "There's a couple of packages that aim to simplify kernel programming without resorting to array operations (which may result in extraneous kernel launches, more on that in some of the next notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946951a",
   "metadata": {},
   "source": [
    "### Tullio.jl\n",
    "\n",
    "With Tullio, it's easy to write kernels using index notation. This makes it easy to express operations like our RMSE calculation in a single expression which typically will also be compiled to a single kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b3b960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Matrix{Float64}:\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Tullio\n",
    "\n",
    "A = ones(10, 10)\n",
    "\n",
    "# assigning with `:=` creates a new array\n",
    "@tullio C[i,j] := A[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6296c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping an index will sum across that dimension\n",
    "@tullio C[i] := A[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb0c96ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-dimensional Array{Float64, 0}:\n",
       "2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe operators can be used to apply functions 'outside' of the reduction\n",
    "@tullio C[] := A[i,j] |> log10(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d63901",
   "metadata": {},
   "source": [
    "With that explained, try to implement the matrix RMSE operation using index notation. Recall the original operation (first on CPU arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29407d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4083704468952621"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(1024, 1024)\n",
    "B = rand(1024, 1024)\n",
    "sqrt(sum((A-B).^2)/length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8fe8b4",
   "metadata": {},
   "source": [
    "Now with Tullio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f183267d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-dimensional Array{Float64, 0}:\n",
       "0.4085763601419757"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tullio C[] := (A[i,j] - B[i,j])^2 |> sqrt(_ / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc4201",
   "metadata": {},
   "source": [
    "To use Tullio with GPU arrays, you need to install and import the relevant CUDA support packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc9b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions, CUDAKernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8463dec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-dimensional CuArray{Float32, 0, CUDA.Mem.DeviceBuffer}:\n",
       "0.40774086"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.rand(1024, 1024)\n",
    "B = CUDA.rand(1024, 1024)\n",
    "@tullio C[] := (A[i,j] - B[i,j])^2 |> sqrt(_ / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed8649",
   "metadata": {},
   "source": [
    "Tullio is great for quickly creating portable kernels (CPU, different GPU back-ends) for mathematical operations, and it can be seen as a generalization of broadcast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b863a6",
   "metadata": {},
   "source": [
    "### KernelAbstractions.jl\n",
    "\n",
    "For a more flexible API, i.e. not restricted to Tullio's index notation, but still retaining Tullio's portability, you can consider the KernelAbstractions.jl framework that Tullio.jl is built on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d513082",
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41110dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel function ka_kernel(A)\n",
    "    # simple kernel without multiple blocks\n",
    "    i = @index(Global, Linear)\n",
    "    \n",
    "    # first thread sets up the data\n",
    "    if i == 1\n",
    "        A[1] = 42\n",
    "    end\n",
    "    \n",
    "    @synchronize()\n",
    "    \n",
    "    # other threads can now read this data\n",
    "    if i != 1\n",
    "        A[i] = A[1]\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb65a2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 42.0\n",
       "  0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = zeros(512)\n",
    "\n",
    "the_ka_kernel = ka_kernel(CPU(), 16)\n",
    "event = the_ka_kernel(A, ndrange=size(A))\n",
    "wait(event)\n",
    "unique(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2aee9",
   "metadata": {},
   "source": [
    "The programming interface is now much closer to CUDA.jl's, while retaining platform portability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "448c0832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 42.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.zeros(512)\n",
    "the_ka_kernel = ka_kernel(CUDADevice(), 16)\n",
    "event = the_ka_kernel(A, ndrange=size(A))\n",
    "wait(event)\n",
    "unique(Array(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252a3bf",
   "metadata": {},
   "source": [
    "The disadvantage of platform portability of course is that KernelAbstraction.jl's feature set is limited to the common denominator of all supported platforms. That means many CUDA features, like atomics or warp-level programming, are not supported. In addition, KernelAbstractions is built on Cassette.jl which will incur a significant compilation cost for nontrivial applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee6a32",
   "metadata": {},
   "source": [
    "## Exercise: Batched matrix RMSE\n",
    "\n",
    "To extend our RMSE example to something more interesting (that we will use in later notebooks), let's extend the computation of the RMSE between two matrices to a batched version that computes `N` RMSEs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2249833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16\n",
    "A = CUDA.rand(1024, 1024, N)\n",
    "B = CUDA.rand(1024, 1024, N)\n",
    "CUDA.allowscalar(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "913fadfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element Vector{Float64}:\n",
       " 0.40795084834098816\n",
       " 0.40852177143096924\n",
       " 0.4084704518318176\n",
       " 0.40801483392715454\n",
       " 0.40796399116516113\n",
       " 0.40854036808013916\n",
       " 0.40823894739151\n",
       " 0.4082832932472229\n",
       " 0.40866413712501526\n",
       " 0.40797749161720276\n",
       " 0.40818750858306885\n",
       " 0.4085010290145874\n",
       " 0.4081467092037201\n",
       " 0.40812161564826965\n",
       " 0.4080013632774353\n",
       " 0.40862977504730225"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(A, B) = sqrt(sum((A-B).^2)/length(A))\n",
    "\n",
    "rmses = Vector{Float64}(undef, N)\n",
    "for i in 1:N\n",
    "    rmses[i] = rmse(A[:, :, i], B[:, :, i])\n",
    "end\n",
    "rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f1e57",
   "metadata": {},
   "source": [
    "This is a pretty bad implementation, but we'll have a look at optimizing it in a future notebook. For now, let's just focus on a correct implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766193a",
   "metadata": {},
   "source": [
    "First, let's try to extend the Tullio expression to correctly handle the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49a01435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.40783513\n",
       " 0.408409\n",
       " 0.40835476\n",
       " 0.40789962\n",
       " 0.40784463\n",
       " 0.40842524\n",
       " 0.40812257\n",
       " 0.4081658\n",
       " 0.4085509\n",
       " 0.4078624\n",
       " 0.40807164\n",
       " 0.40837982\n",
       " 0.40803325\n",
       " 0.4080107\n",
       " 0.40788764\n",
       " 0.40851375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tullio C[k] := (A[i,j,k] - B[i,j,k])^2 |> sqrt(_ / (size(A,1)*size(A,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744937f4",
   "metadata": {},
   "source": [
    "Note the manual length computation because Tullio doesn't like an additional `prod(size(A)[1:2])`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf39d59",
   "metadata": {},
   "source": [
    "Next, try to extend the grid-stride kernel implementation to handle multiple batches. We could just launch our kernel `N` times, but let's try and handle the batching *inside* the kernel. The easiest way to do so, is to launch one block per batch and to fetch the batch number inside the kernel from the `blockIdx()` hardware indices.\n",
    "\n",
    "That poses a problem though, as we were using a linear index whereas we now need 3 indices (x, y, and batch). There's multiple possible solutions:\n",
    "- generalize indexing to cartesian indices\n",
    "- launch 2-dimensional blocks, and extend the grid-stride loop to cover both dimensions\n",
    "- reshape the input to a 2D matrix (i.e. flatten the matrix dimensions)\n",
    "\n",
    "Let's start with reshaping, for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68295ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_flat = reshape(A, (prod(size(A)[1:2]),N))\n",
    "B_flat = reshape(B, (prod(size(B)[1:2]),N))\n",
    "C = similar(A, N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbfa9594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.40783507\n",
       " 0.408409\n",
       " 0.40835464\n",
       " 0.40789986\n",
       " 0.40784472\n",
       " 0.40842536\n",
       " 0.40812278\n",
       " 0.40816572\n",
       " 0.40855077\n",
       " 0.40786234\n",
       " 0.4080718\n",
       " 0.40837958\n",
       " 0.40803325\n",
       " 0.40801066\n",
       " 0.407888\n",
       " 0.40851375"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B)  \n",
    "    batch = blockIdx().x\n",
    "\n",
    "    # initialize the memory\n",
    "    if threadIdx().x == 1\n",
    "        C[batch] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # grid-stride loop to process each batch in a block\n",
    "    for i in threadIdx().x:blockDim().x:size(A,1)\n",
    "        a = A[i, batch]\n",
    "        b = B[i, batch]\n",
    "        CUDA.@atomic C[batch] += (a-b)^2\n",
    "    end    \n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if threadIdx().x == 1\n",
    "        C[batch] = sqrt(C[batch] / size(A,1))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=256 blocks=N rmse_kernel(C, A_flat, B_flat)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64a60c",
   "metadata": {},
   "source": [
    "A much more general pattern for dealing with multiple independent datasets or batches within a single kernel (i.e. without launching multiple kernels, one for each batch, or without reshaping data) so is to compute and pass separate cartesian indices to the kernel, and make sure those map into hardware indices the way we want. For example, here we have N-dimensional inputs whose last index represents the batch, so we can pass two separate cartesian indices:\n",
    "- one representing the 'main' iteration space, where the last index doesn't count\n",
    "- one representing the batches, having the samen dimensionality, but with only the last index set\n",
    "\n",
    "As we want each RMSE calculation between arrays from a single batch to happen within a single block (again, to simplify communication and synchronization), we should index the main cartesian indices object using a thread index, while using a block index for the batch indices. Within the kernel, we can then merge these two objects using the `max` operator to get a usable index. For more information on this technique, refer to the following blog post: https://julialang.org/blog/2016/02/iteration/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc242b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024×1024×1 CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}:\n",
       "[:, :, 1] =\n",
       " CartesianIndex(1, 1, 1)     …  CartesianIndex(1, 1024, 1)\n",
       " CartesianIndex(2, 1, 1)        CartesianIndex(2, 1024, 1)\n",
       " CartesianIndex(3, 1, 1)        CartesianIndex(3, 1024, 1)\n",
       " CartesianIndex(4, 1, 1)        CartesianIndex(4, 1024, 1)\n",
       " CartesianIndex(5, 1, 1)        CartesianIndex(5, 1024, 1)\n",
       " CartesianIndex(6, 1, 1)     …  CartesianIndex(6, 1024, 1)\n",
       " CartesianIndex(7, 1, 1)        CartesianIndex(7, 1024, 1)\n",
       " CartesianIndex(8, 1, 1)        CartesianIndex(8, 1024, 1)\n",
       " CartesianIndex(9, 1, 1)        CartesianIndex(9, 1024, 1)\n",
       " CartesianIndex(10, 1, 1)       CartesianIndex(10, 1024, 1)\n",
       " CartesianIndex(11, 1, 1)    …  CartesianIndex(11, 1024, 1)\n",
       " CartesianIndex(12, 1, 1)       CartesianIndex(12, 1024, 1)\n",
       " CartesianIndex(13, 1, 1)       CartesianIndex(13, 1024, 1)\n",
       " ⋮                           ⋱  \n",
       " CartesianIndex(1013, 1, 1)     CartesianIndex(1013, 1024, 1)\n",
       " CartesianIndex(1014, 1, 1)     CartesianIndex(1014, 1024, 1)\n",
       " CartesianIndex(1015, 1, 1)     CartesianIndex(1015, 1024, 1)\n",
       " CartesianIndex(1016, 1, 1)  …  CartesianIndex(1016, 1024, 1)\n",
       " CartesianIndex(1017, 1, 1)     CartesianIndex(1017, 1024, 1)\n",
       " CartesianIndex(1018, 1, 1)     CartesianIndex(1018, 1024, 1)\n",
       " CartesianIndex(1019, 1, 1)     CartesianIndex(1019, 1024, 1)\n",
       " CartesianIndex(1020, 1, 1)     CartesianIndex(1020, 1024, 1)\n",
       " CartesianIndex(1021, 1, 1)  …  CartesianIndex(1021, 1024, 1)\n",
       " CartesianIndex(1022, 1, 1)     CartesianIndex(1022, 1024, 1)\n",
       " CartesianIndex(1023, 1, 1)     CartesianIndex(1023, 1024, 1)\n",
       " CartesianIndex(1024, 1, 1)     CartesianIndex(1024, 1024, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rmain = ntuple(i->i == ndims(A) ? Base.OneTo(1) : axes(A)[i], ndims(A)) |> CartesianIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9ee8b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1×16 CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}:\n",
       "[:, :, 1] =\n",
       " CartesianIndex(1, 1, 1)\n",
       "\n",
       "[:, :, 2] =\n",
       " CartesianIndex(1, 1, 2)\n",
       "\n",
       "[:, :, 3] =\n",
       " CartesianIndex(1, 1, 3)\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 14] =\n",
       " CartesianIndex(1, 1, 14)\n",
       "\n",
       "[:, :, 15] =\n",
       " CartesianIndex(1, 1, 15)\n",
       "\n",
       "[:, :, 16] =\n",
       " CartesianIndex(1, 1, 16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rbatch = ntuple(i->i != ndims(A) ? Base.OneTo(1) : axes(A)[i], ndims(A)) |> CartesianIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "112a6f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.40783522\n",
       " 0.40840903\n",
       " 0.40835467\n",
       " 0.40790036\n",
       " 0.40784442\n",
       " 0.40842506\n",
       " 0.40812257\n",
       " 0.40816554\n",
       " 0.40855056\n",
       " 0.4078624\n",
       " 0.40807194\n",
       " 0.40838\n",
       " 0.40803307\n",
       " 0.40801063\n",
       " 0.40788803\n",
       " 0.40851396"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B, Rmain, Rbatch)\n",
    "    batch = blockIdx().x\n",
    "    Ibatch = Rbatch[batch]\n",
    "    \n",
    "    # initialize the memory\n",
    "    if threadIdx().x == 1\n",
    "        C[batch] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # grid-stride loop to process each batch in a block\n",
    "    for i in threadIdx().x:blockDim().x:length(Rmain)\n",
    "        Imain = Rmain[i]\n",
    "        I = max(Imain, Ibatch)\n",
    "        a = A[I]\n",
    "        b = B[I]\n",
    "        CUDA.@atomic C[batch] += (a-b)^2\n",
    "    end    \n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if threadIdx().x == 1\n",
    "        C[batch] = sqrt(C[batch] / length(Rmain))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=256 blocks=N rmse_kernel(C, A, B, Rmain, Rbatch)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b022011",
   "metadata": {},
   "source": [
    "We now have a fully general kernel that handles arbitrarily-sized inputs, treating the last dimension as the batch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
