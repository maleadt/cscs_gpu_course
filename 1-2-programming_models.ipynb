{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8435688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/Julia/doc/cscs_gpu_course/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a459ca",
   "metadata": {},
   "source": [
    "# Programming models\n",
    "\n",
    "There are different ways of programming (NVIDIA) GPUs in Julia, at different levels of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e37034",
   "metadata": {},
   "source": [
    "## Array programming\n",
    "\n",
    "The easiest way to use a GPU is via vectorized array operations. Each of these operations will be backed by one or more GPU kernels, so as long as your data is large enough you'll get to see some nice speed-ups. For NVIDIA GPUs, you use the `CuArray` type from CUDA.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae46b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "CuArray([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3ec71",
   "metadata": {},
   "source": [
    "A `CuArray` is an object that can be used on the CPU, representing a chunk of memory on the GPU. This is important: All operations on a `CuArray` are CPU methods which launch on or more GPU kernels operating on the values in GPU memory. This has several consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57652dee",
   "metadata": {},
   "source": [
    "### Scalar iteration is slow\n",
    "\n",
    "Because `CuArray` operations start on the CPU, the array operations should be relatively heavyweight to offset the overhead it takes to launch one or more GPU operations. That means that a scalar `for` loop processing one element at a time is very wasteful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba2b8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar indexing on task Task (runnable) @0x00007fc5b0941510.\n",
      "│ Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "│ This is typically caused by calling an iterating implementation of a method.\n",
      "│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "│ and therefore are only permitted from the REPL for prototyping purposes.\n",
      "│ If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "└ @ GPUArrays /home/tim/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CuArray(1:10)\n",
    "A_sum = zero(eltype(A))\n",
    "for I in eachindex(A)\n",
    "    A_sum += A[I]\n",
    "end\n",
    "A_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee038f2c",
   "metadata": {},
   "source": [
    "Because of this kind of programming pattern, iterating the array and fetching one scalar at a time (hence 'scalar iteration'), being so slow CUDA.jl warns about it. With the above snippet, the situation is actually even worse: Not only does every iteration require a GPU operation to fetch an element, the `getindex` call is also the only array operation meaning that the actual summation won't even run on the GPU!\n",
    "\n",
    "The solution here is to use the `sum` function that performs the entire operation as a single step. More on these operations later.\n",
    "To disallow scalar iteration, use the `allowscalar` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303d3f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:33",
      " [2] assertscalar(op::String)",
      "   @ GPUArrays ~/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:53",
      " [3] getindex(xs::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}, I::Int64)",
      "   @ GPUArrays ~/Julia/depot/packages/GPUArrays/3sW6s/src/host/indexing.jl:86",
      " [4] top-level scope",
      "   @ In[3]:2",
      " [5] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [6] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "CUDA.allowscalar(false)\n",
    "A[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c612d",
   "metadata": {},
   "source": [
    "### CuArray isn't device-compatible\n",
    "\n",
    "A more subtle result of `CuArray` being the CPU-side object is that these objects cannot be used directly on the GPU. Instead, a conversion to `CuDeviceArray` happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd79553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTX CompilerJob of kernel #2(CuDeviceVector{Int64, 1}) for sm_75\n",
      "\n",
      "Variables\n",
      "  #self#\u001b[36m::Core.Const(var\"#2#3\"())\u001b[39m\n",
      "  A\u001b[36m::CuDeviceVector{Int64, 1}\u001b[39m\n",
      "\n",
      "Body\u001b[36m::Nothing\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m     return Main.nothing\n"
     ]
    }
   ],
   "source": [
    "@device_code_warntype @cuda (A->nothing)(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8fd2d",
   "metadata": {},
   "source": [
    "Typically, this conversion is hidden and shouldn't affect you as an end user. The only time you need to take care, is when embedding `CuArray`s in a structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c30c38c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "GPU compilation of kernel #4(MyStruct) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type MyStruct, which is not isbits:\n  .inner is of type CuArray which is not isbits.\n    .storage is of type Union{Nothing, CUDA.ArrayStorage{B}} where B which is not isbits.\n    .dims is of type Tuple{Vararg{Int64, N}} where N which is not isbits.\n\n",
     "output_type": "error",
     "traceback": [
      "GPU compilation of kernel #4(MyStruct) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type MyStruct, which is not isbits:\n  .inner is of type CuArray which is not isbits.\n    .storage is of type Union{Nothing, CUDA.ArrayStorage{B}} where B which is not isbits.\n    .dims is of type Tuple{Vararg{Int64, N}} where N which is not isbits.\n\n",
      "",
      "Stacktrace:",
      "  [1] check_invocation(job::GPUCompiler.CompilerJob)",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/validation.jl:66",
      "  [2] macro expansion",
      "    @ ~/Julia/depot/packages/GPUCompiler/AJD5L/src/driver.jl:332 [inlined]",
      "  [3] macro expansion",
      "    @ ~/Julia/depot/packages/TimerOutputs/SSeq1/src/TimerOutput.jl:252 [inlined]",
      "  [4] macro expansion",
      "    @ ~/Julia/depot/packages/GPUCompiler/AJD5L/src/driver.jl:331 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/utils.jl:62",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:326",
      "  [7] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/Julia/depot/packages/GPUCompiler/AJD5L/src/cache.jl:89",
      "  [8] cufunction(f::var\"#4#5\", tt::Type{Tuple{MyStruct}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:297",
      "  [9] cufunction(f::var\"#4#5\", tt::Type{Tuple{MyStruct}})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:291",
      " [10] top-level scope",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:102",
      " [11] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [12] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "struct MyStruct\n",
    "    inner::CuArray\n",
    "end\n",
    "B = MyStruct(A)\n",
    "@cuda (A->nothing)(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f4aaa",
   "metadata": {},
   "source": [
    "Here, CUDA.jl makes it clear that a `CuArray` isn't GPU compatible because it's not an `isbits` type. The underlying reason is that the automatic conversion from `CuArray` to `CuDeviceArray` doesn't know about your `MyStruct` and how to convert it to something GPU-compatible. This conversion is done using Adapt.jl, and to make this code work you need to teach Adapt about how to convert `MyStruct` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccabb456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTX CompilerJob of kernel #6(MyParametricStruct{CuDeviceVector{Int64, 1}}) for sm_75\n",
      "\n",
      "Variables\n",
      "  #self#\u001b[36m::Core.Const(var\"#6#7\"())\u001b[39m\n",
      "  A\u001b[36m::MyParametricStruct{CuDeviceVector{Int64, 1}}\u001b[39m\n",
      "\n",
      "Body\u001b[36m::Nothing\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m     return Main.nothing\n"
     ]
    }
   ],
   "source": [
    "# to store both a CuArray and a CuDeviceArray\n",
    "# our struct needs to be parametric\n",
    "struct MyParametricStruct{T<:AbstractArray}\n",
    "    inner::T\n",
    "end\n",
    "\n",
    "using Adapt\n",
    "Adapt.adapt_structure(to, x::MyParametricStruct) = MyParametricStruct(adapt(to, x.inner))\n",
    "\n",
    "C = MyParametricStruct(A)\n",
    "@device_code_warntype @cuda (A->nothing)(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d3543",
   "metadata": {},
   "source": [
    "### Array operations\n",
    "\n",
    "Many array operations from Julia's standard library and package ecosystem Just Work, because they rely on the `AbstractArray` interfaces that are implemented by `CuArray`. Still, CUDA.jl specializes a bunch of methods for several reasons:\n",
    "\n",
    "- compatibility: to avoid scalar iteration, or calling into a C library with CPU code (e.g. BLAS, LAPACK, ...)\n",
    "- performance: by using GPU-optimized implementations, either implemented in Julia or in vendor libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c753e85",
   "metadata": {},
   "source": [
    "If an array operation isn't available, two kinds of errors might occur:\n",
    "\n",
    "- scalar iteration: when the default implementation processes individual elements at a time\n",
    "- invalid conversions to a CPU pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c6005b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: cannot take the CPU address of a CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}",
     "output_type": "error",
     "traceback": [
      "ArgumentError: cannot take the CPU address of a CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}",
      "",
      "Stacktrace:",
      " [1] unsafe_convert(#unused#::Type{Ptr{Int64}}, x::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer})",
      "   @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/array.jl:315",
      " [2] unsafe_convert(#unused#::Type{Ptr{Float32}}, a::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer})",
      "   @ Base ./pointer.jl:66",
      " [3] top-level scope",
      "   @ ./In[7]:1",
      " [4] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "ccall(:whatever, Nothing, (Ptr{Float32},), A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c3981",
   "metadata": {},
   "source": [
    "In that case, either you need to use different (supported) array operations, or fix the implementation in CUDA.jl. Such a fix can mean using functions from a CUDA library, using existing operations, or writing your own kernel."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c166fc3e",
   "metadata": {},
   "source": [
    "### Exercise: Matrix RMSE\n",
    "\n",
    "As an trivial exercise, try to compute the RMSE of two matrices on the GPU:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ee738d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4083871458683567"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(1024, 1024)\n",
    "B = rand(1024, 1024)\n",
    "sqrt(sum((A-B).^2) / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50845c3",
   "metadata": {},
   "source": [
    "Easy enough, just changing the type of the input arrays to `CuArray` and the computation of C just works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1defb88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4083871458683567"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CuArray(A)\n",
    "B = CuArray(B)\n",
    "sqrt(sum((A-B).^2) / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1447015",
   "metadata": {},
   "source": [
    "This is of course a trivial example, but let's keep it simple for now. The next notebooks will start from this example and create something realistic out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecb474",
   "metadata": {},
   "source": [
    "## Kernel programming\n",
    "\n",
    "Kernels are scalar functions that get executed multiple times in parallel. Each 'thread' runs on one of the many streaming multiprocessors a GPU has, and threads running on a single SM are called a 'block'. Within a SM, some threads are always executed together; these form a 'warp' of 32 threads. Efficient communication between these entities is required to effectively use the GPU:\n",
    "\n",
    "- between blocks: global memory\n",
    "- within a block: shared memory\n",
    "- within a warp: via registers (shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e4c71",
   "metadata": {},
   "source": [
    "### Hardware indices\n",
    "\n",
    "You can fetch the thread, block and warp index using specific functions that query hardware indices:\n",
    "\n",
    "- `threadIdx()` and `blockDim()`: 3D\n",
    "- `blockIdx()` and `gridDim()`: 3D\n",
    "- `laneid()` and `warpsize()`\n",
    "\n",
    "When you don't need to care about which block a thread is part of, a very common index calculation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf386c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel{typeof(kernel), Tuple{}}(kernel, CuContext(0x00000000062264a0, instance f02fca961a36c4ad), CuModule(Ptr{Nothing} @0x0000000009d56840, CuContext(0x00000000062264a0, instance f02fca961a36c4ad)), CuFunction(Ptr{Nothing} @0x00000000094ce9c0, CuModule(Ptr{Nothing} @0x0000000009d56840, CuContext(0x00000000062264a0, instance f02fca961a36c4ad))), CUDA.KernelState(Ptr{Nothing} @0x00007fc560c00000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1\n",
      "i = 2\n",
      "i = 3\n",
      "i = 4\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @cushow i\n",
    "    return\n",
    "end\n",
    "@cuda threads=2 blocks=2 kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4247070",
   "metadata": {},
   "source": [
    "### Synchronization\n",
    "\n",
    "If threads are working together -- say, they are using the same global memory, or are communicating using shared memory or finer-grained intrinsics -- you may want to have threads on each other. Note that this is only possible **within a block**; different blocks generally cannot wait on one another.\n",
    "\n",
    "Let's look at a contrived example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ff4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 42.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.zeros(512)\n",
    "\n",
    "function kernel(A)\n",
    "    # simple kernel without multiple blocks\n",
    "    i = threadIdx().x\n",
    "    \n",
    "    # first thread sets up the data\n",
    "    if i == 1\n",
    "        A[1] = 42\n",
    "    end\n",
    "    \n",
    "    sync_threads()\n",
    "    \n",
    "    # other threads can now read this data\n",
    "    if i != 1\n",
    "        A[i] = A[1]\n",
    "    end\n",
    "    \n",
    "    return\n",
    "end\n",
    "@cuda threads=length(A) kernel(A)\n",
    "unique(Array(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f23cc",
   "metadata": {},
   "source": [
    "Note how we didn't put `sync_threads()` inside of the branch; All threads need to reach the synchronization point for the kernel to make progress. This makes it dangerous to synchronize from a branch, as the branch cannot be divergent within a block or the kernel would deadlock!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f21b3",
   "metadata": {},
   "source": [
    "When coordinating within the warp, you may need the `sync_warp()` function. A detailed explanation of warp-level programming is out of scope for this notebook, refer to the [NVIDIA developer blog](https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade663de",
   "metadata": {},
   "source": [
    "### Atomic operations\n",
    "\n",
    "When you want to use the same global memory from different threads, you may want to use atomic operations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb9d0979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255.92522f0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum = CUDA.zeros(1)\n",
    "A = CUDA.rand(512)\n",
    "\n",
    "function kernel(A, A_sum)\n",
    "    i = threadIdx().x\n",
    "    CUDA.@atomic A_sum[] += A[i]\n",
    "    return\n",
    "end\n",
    "@cuda threads=length(A) kernel(A, A_sum)\n",
    "Array(A_sum)[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3e656",
   "metadata": {},
   "source": [
    "You shouldn't overuse atomics though, as they generally serialize execution and thus are very expensive! But they may be useful for an initial implementation (i.e. before considering more fine-grained communication), or to reduce values from different blocks (because of the difficulty of synchronizing the grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5e5b7",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "To help with implementing a kernel, there's a couple of helpful macros to generate output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f880a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "function kernel()\n",
    "    i = threadIdx().x\n",
    "    @cuprintf \"I'm thread %ld\\n\" Int(i)\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cfed74",
   "metadata": {},
   "source": [
    "However, `@cuprintf` is a bit cumbersome, so we have `@cuprintln` trying to automatically generate an appropriate formatting string, while even supporting string interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0804fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm thread 1\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = threadIdx().x\n",
    "    @cuprintln \"I'm thread $i\"\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6add9d3",
   "metadata": {},
   "source": [
    "And for quick debugging, we have a helpful `@cushow` you can surround expressions with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110e15db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm thread 1\n"
     ]
    }
   ],
   "source": [
    "function kernel()\n",
    "    i = @cushow(threadIdx().x)\n",
    "    return\n",
    "end\n",
    "@cuda kernel();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97414d",
   "metadata": {},
   "source": [
    "### Exercise: Matrix RMSE kernel\n",
    "\n",
    "We now have all the pieces needed to port our RMSE calculation to a kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa8342d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42389724f0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.rand(10,10)\n",
    "B = CUDA.rand(10,10)\n",
    "sqrt(sum((A-B).^2)/length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8d1cf",
   "metadata": {},
   "source": [
    "Try to write a kernel that takes a single-item output array as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "005711b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " -0.28540277"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = CUDA.similar(A, 1)\n",
    "\n",
    "function rmse_kernel(C, A, B)\n",
    "    # ...\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=length(A) rmse_kernel(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e2abb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.42389724"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B)  \n",
    "    i = threadIdx().x\n",
    "\n",
    "    # initialize the memory\n",
    "    if i == 1\n",
    "        C[] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # process an element on each thread\n",
    "    a = A[i]\n",
    "    b = B[i]\n",
    "    CUDA.@atomic C[] += (a-b)^2\n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if i == 1\n",
    "        C[1] = sqrt(C[] / length(A))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=length(A) rmse_kernel(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0450ab",
   "metadata": {},
   "source": [
    "This kernel only works when the array fits in a single block though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "273206e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
     "output_type": "error",
     "traceback": [
      "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
      "",
      "Stacktrace:",
      "  [1] throw_api_error(res::CUDA.cudaError_enum)",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/error.jl:91",
      "  [2] macro expansion",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/error.jl:101 [inlined]",
      "  [3] cuLaunchKernel(f::CuFunction, gridDimX::UInt32, gridDimY::UInt32, gridDimZ::UInt32, blockDimX::UInt32, blockDimY::UInt32, blockDimZ::UInt32, sharedMemBytes::Int64, hStream::CuStream, kernelParams::Vector{Ptr{Nothing}}, extra::Ptr{Nothing})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/lib/utils/call.jl:26",
      "  [4] #27",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:69 [inlined]",
      "  [5] macro expansion",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:33 [inlined]",
      "  [6] macro expansion",
      "    @ ./none:0 [inlined]",
      "  [7] pack_arguments(::CUDA.var\"#27#28\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3}, ::CUDA.KernelState, ::CuDeviceVector{Float32, 1}, ::CuDeviceMatrix{Float32, 1}, ::CuDeviceMatrix{Float32, 1})",
      "    @ CUDA ./none:0",
      "  [8] #launch#26",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:62 [inlined]",
      "  [9] #32",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:136 [inlined]",
      " [10] macro expansion",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:95 [inlined]",
      " [11] macro expansion",
      "    @ ./none:0 [inlined]",
      " [12] convert_arguments",
      "    @ ./none:0 [inlined]",
      " [13] #cudacall#31",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/lib/cudadrv/execution.jl:135 [inlined]",
      " [14] macro expansion",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:204 [inlined]",
      " [15] macro expansion",
      "    @ ./none:0 [inlined]",
      " [16] call(::CUDA.HostKernel{typeof(rmse_kernel), Tuple{CuDeviceVector{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}}, ::CuDeviceVector{Float32, 1}, ::CuDeviceMatrix{Float32, 1}, ::CuDeviceMatrix{Float32, 1}; call_kwargs::Base.Iterators.Pairs{Symbol, Int64, Tuple{Symbol, Symbol}, NamedTuple{(:threads, :blocks), Tuple{Int64, Int64}}})",
      "    @ CUDA ./none:0",
      " [17] (::CUDA.HostKernel{typeof(rmse_kernel), Tuple{CuDeviceVector{Float32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}})(::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, ::Vararg{Any, N} where N; threads::Int64, blocks::Int64, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:462",
      " [18] top-level scope",
      "    @ ~/Julia/depot/packages/CUDA/P3AyQ/src/compiler/execution.jl:104",
      " [19] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [20] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "A = CUDA.rand(1024,1024)\n",
    "B = CUDA.rand(1024,1024)\n",
    "@cuda threads=length(A) rmse_kernel(C, A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a151df",
   "metadata": {},
   "source": [
    "We could just use multiple blocks, since our currently implementation doesn't use any communication between threads. However, in a future notebook we _will_ add such communication, so for now let's make it so that we only need a single block to process this matrix.\n",
    "\n",
    "A good way to do so is to introduce a grid-stride loop. This has been explained in a previous notebook, so try adapting the kernel implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "addd90a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.40774998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B)  \n",
    "    # initialize the memory\n",
    "    if threadIdx().x == 1\n",
    "        C[1] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # grid-stride loop to process each batch in a block\n",
    "    for i in threadIdx().x:blockDim().x:length(A)\n",
    "        a = A[i]\n",
    "        b = B[i]\n",
    "        CUDA.@atomic C[1] += (a-b)^2\n",
    "    end    \n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if threadIdx().x == 1\n",
    "        C[1] = sqrt(C[1] / length(A))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=256 rmse_kernel(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e4dba",
   "metadata": {},
   "source": [
    "## High-level kernel programming\n",
    "\n",
    "There's a couple of packages that aim to simplify kernel programming without resorting to array operations (which may result in extraneous kernel launches, more on that in some of the next notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3946951a",
   "metadata": {},
   "source": [
    "### Tullio.jl\n",
    "\n",
    "With Tullio, it's easy to write kernels using index notation. This makes it easy to express operations like our RMSE calculation in a single expression which typically will also be compiled to a single kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b3b960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10 Matrix{Float64}:\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       " 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Tullio\n",
    "\n",
    "A = ones(10, 10)\n",
    "\n",
    "# assigning with `:=` creates a new array\n",
    "@tullio C[i,j] := A[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6296c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0\n",
       " 10.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping an index will sum across that dimension\n",
    "@tullio C[i] := A[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb0c96ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-dimensional Array{Float64, 0}:\n",
       "2.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe operators can be used to apply functions 'outside' of the reduction\n",
    "@tullio C[] := A[i,j] |> log10(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d63901",
   "metadata": {},
   "source": [
    "#### Exercise: Matrix RMSE with index notation\n",
    "\n",
    "With these bits, try to create an index notation expression that computes the RMSE of two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29407d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4085763601419784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Tullio\n",
    "\n",
    "A = rand(1024, 1024)\n",
    "B = rand(1024, 1024)\n",
    "sqrt(sum((A-B).^2)/length(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f183267d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-dimensional Array{Float64, 0}:\n",
       "0.4085763601419757"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tullio C[] := (A[i,j] - B[i,j])^2 |> sqrt(_ / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc4201",
   "metadata": {},
   "source": [
    "To use Tullio with GPU arrays, you need to install and import the relevant CUDA support packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc9b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions, CUDAKernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8463dec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-dimensional CuArray{Float32, 0, CUDA.Mem.DeviceBuffer}:\n",
       "0.40774086"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.rand(1024, 1024)\n",
    "B = CUDA.rand(1024, 1024)\n",
    "@tullio C[] := (A[i,j] - B[i,j])^2 |> sqrt(_ / length(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed8649",
   "metadata": {},
   "source": [
    "Tullio is great for quickly creating portable kernels (CPU, different GPU back-ends) for mathematical operations, and it can be seen as a generalization of broadcast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b863a6",
   "metadata": {},
   "source": [
    "### KernelAbstractions.jl\n",
    "\n",
    "For a more flexible API, i.e. not restricted to Tullio's index notation, but still retaining Tullio's portability, you can consider the KernelAbstractions.jl framework that Tullio.jl is built on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d513082",
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41110dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kernel function ka_kernel(A)\n",
    "    # simple kernel without multiple blocks\n",
    "    i = @index(Global, Linear)\n",
    "    \n",
    "    # first thread sets up the data\n",
    "    if i == 1\n",
    "        A[1] = 42\n",
    "    end\n",
    "    \n",
    "    @synchronize()\n",
    "    \n",
    "    # other threads can now read this data\n",
    "    if i != 1\n",
    "        A[i] = A[1]\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb65a2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 42.0\n",
       "  0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = zeros(512)\n",
    "\n",
    "the_ka_kernel = ka_kernel(CPU(), 16)\n",
    "event = the_ka_kernel(A, ndrange=size(A))\n",
    "wait(event)\n",
    "unique(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2aee9",
   "metadata": {},
   "source": [
    "The programming interface is now much closer to CUDA.jl's, while retaining platform portability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "448c0832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float32}:\n",
       " 42.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = CUDA.zeros(512)\n",
    "the_ka_kernel = ka_kernel(CUDADevice(), 16)\n",
    "event = the_ka_kernel(A, ndrange=size(A))\n",
    "wait(event)\n",
    "unique(Array(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252a3bf",
   "metadata": {},
   "source": [
    "The disadvantage of platform portability of course is that KernelAbstraction.jl's feature set is limited to the common denominator of all supported platforms. That means many CUDA features, like atomics or warp-level programming, are not supported. In addition, KernelAbstractions is built on Cassette.jl which will incur a significant compilation cost for nontrivial applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee6a32",
   "metadata": {},
   "source": [
    "## Exercise: Batched matrix RMSE\n",
    "\n",
    "To extend our RMSE example to something more interesting (that we will use in later notebooks), let's extend the computation of the RMSE between two matrices to a batched version that computes `N` RMSEs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2249833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16\n",
    "A = CUDA.rand(1024, 1024, N)\n",
    "B = CUDA.rand(1024, 1024, N)\n",
    "CUDA.allowscalar(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "913fadfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element Vector{Float64}:\n",
       " 0.40795084834098816\n",
       " 0.40852177143096924\n",
       " 0.4084704518318176\n",
       " 0.40801483392715454\n",
       " 0.40796399116516113\n",
       " 0.40854036808013916\n",
       " 0.40823894739151\n",
       " 0.4082832932472229\n",
       " 0.40866413712501526\n",
       " 0.40797749161720276\n",
       " 0.40818750858306885\n",
       " 0.4085010290145874\n",
       " 0.4081467092037201\n",
       " 0.40812161564826965\n",
       " 0.4080013632774353\n",
       " 0.40862977504730225"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(A, B) = sqrt(sum((A-B).^2)/length(A))\n",
    "\n",
    "rmses = Vector{Float64}(undef, N)\n",
    "for i in 1:N\n",
    "    rmses[i] = rmse(A[:, :, i], B[:, :, i])\n",
    "end\n",
    "rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f1e57",
   "metadata": {},
   "source": [
    "This is a pretty bad implementation, but we'll have a look at optimizing it in a future notebook. For now, let's just focus on the implementation.\n",
    "\n",
    "First, let's try to extend the Tullio expression to correctly handle the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49a01435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.40783513\n",
       " 0.408409\n",
       " 0.40835476\n",
       " 0.40789962\n",
       " 0.40784463\n",
       " 0.40842524\n",
       " 0.40812257\n",
       " 0.4081658\n",
       " 0.4085509\n",
       " 0.4078624\n",
       " 0.40807164\n",
       " 0.40837982\n",
       " 0.40803325\n",
       " 0.4080107\n",
       " 0.40788764\n",
       " 0.40851375"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tullio C[k] := (A[i,j,k] - B[i,j,k])^2 |> sqrt(_ / (size(A,1)*size(A,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744937f4",
   "metadata": {},
   "source": [
    "Note the manual length computation because Tullio doesn't like an additional `prod(size(A)[1:2])`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf39d59",
   "metadata": {},
   "source": [
    "Next, try to extend the grid-stride kernel implementation to handle multiple batches. We could just launch our kernel `N` times, but let's try and handle the batching *inside* the kernel. The easiest way to do so, is to launch one block per batch and to fetch the batch number inside the kernel from the `blockIdx()` hardware indices.\n",
    "\n",
    "That poses a problem though, as we were using a linear index whereas we now need 3 indices (x, y, and batch). There's multiple possible solutions:\n",
    "- generalize indexing to cartesian indices\n",
    "- launch 2-dimensional blocks, and extend the grid-stride loop to cover both dimensions\n",
    "- reshape the input to a 2D matrix (i.e. flatten the matrix dimensions)\n",
    "\n",
    "Let's start with reshaping, for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68295ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_flat = reshape(A, (prod(size(A)[1:2]),N))\n",
    "B_flat = reshape(B, (prod(size(B)[1:2]),N))\n",
    "C = similar(A, N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbfa9594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.40783507\n",
       " 0.408409\n",
       " 0.40835464\n",
       " 0.40789986\n",
       " 0.40784472\n",
       " 0.40842536\n",
       " 0.40812278\n",
       " 0.40816572\n",
       " 0.40855077\n",
       " 0.40786234\n",
       " 0.4080718\n",
       " 0.40837958\n",
       " 0.40803325\n",
       " 0.40801066\n",
       " 0.407888\n",
       " 0.40851375"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B)  \n",
    "    batch = blockIdx().x\n",
    "\n",
    "    # initialize the memory\n",
    "    if threadIdx().x == 1\n",
    "        C[batch] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # grid-stride loop to process each batch in a block\n",
    "    for i in threadIdx().x:blockDim().x:size(A,1)\n",
    "        a = A[i, batch]\n",
    "        b = B[i, batch]\n",
    "        CUDA.@atomic C[batch] += (a-b)^2\n",
    "    end    \n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if threadIdx().x == 1\n",
    "        C[batch] = sqrt(C[batch] / size(A,1))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=256 blocks=N rmse_kernel(C, A_flat, B_flat)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64a60c",
   "metadata": {},
   "source": [
    "A much more general pattern for dealing with multiple independent datasets or batches within a single kernel (i.e. without launching multiple kernels, one for each batch, or without reshaping data) so is to compute and pass separate cartesian indices to the kernel, and make sure those map into hardware indices the way we want. For example, here we have N-dimensional inputs whose last index represents the batch, so we can pass two separate cartesian indices:\n",
    "- one representing the 'main' iteration space, where the last index doesn't count\n",
    "- one representing the batches, having the samen dimensionality, but with only the last index set\n",
    "\n",
    "As we want each RMSE calculation between arrays from a single batch to happen within a single block (again, to simplify communication and synchronization), we should index the main cartesian indices object using a thread index, while using a block index for the batch indices. Within the kernel, we can then merge these two objects using the `max` operator to get a usable index. For more information on this technique, refer to the following blog post: https://julialang.org/blog/2016/02/iteration/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc242b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024×1024×1 CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}:\n",
       "[:, :, 1] =\n",
       " CartesianIndex(1, 1, 1)     …  CartesianIndex(1, 1024, 1)\n",
       " CartesianIndex(2, 1, 1)        CartesianIndex(2, 1024, 1)\n",
       " CartesianIndex(3, 1, 1)        CartesianIndex(3, 1024, 1)\n",
       " CartesianIndex(4, 1, 1)        CartesianIndex(4, 1024, 1)\n",
       " CartesianIndex(5, 1, 1)        CartesianIndex(5, 1024, 1)\n",
       " CartesianIndex(6, 1, 1)     …  CartesianIndex(6, 1024, 1)\n",
       " CartesianIndex(7, 1, 1)        CartesianIndex(7, 1024, 1)\n",
       " CartesianIndex(8, 1, 1)        CartesianIndex(8, 1024, 1)\n",
       " CartesianIndex(9, 1, 1)        CartesianIndex(9, 1024, 1)\n",
       " CartesianIndex(10, 1, 1)       CartesianIndex(10, 1024, 1)\n",
       " CartesianIndex(11, 1, 1)    …  CartesianIndex(11, 1024, 1)\n",
       " CartesianIndex(12, 1, 1)       CartesianIndex(12, 1024, 1)\n",
       " CartesianIndex(13, 1, 1)       CartesianIndex(13, 1024, 1)\n",
       " ⋮                           ⋱  \n",
       " CartesianIndex(1013, 1, 1)     CartesianIndex(1013, 1024, 1)\n",
       " CartesianIndex(1014, 1, 1)     CartesianIndex(1014, 1024, 1)\n",
       " CartesianIndex(1015, 1, 1)     CartesianIndex(1015, 1024, 1)\n",
       " CartesianIndex(1016, 1, 1)  …  CartesianIndex(1016, 1024, 1)\n",
       " CartesianIndex(1017, 1, 1)     CartesianIndex(1017, 1024, 1)\n",
       " CartesianIndex(1018, 1, 1)     CartesianIndex(1018, 1024, 1)\n",
       " CartesianIndex(1019, 1, 1)     CartesianIndex(1019, 1024, 1)\n",
       " CartesianIndex(1020, 1, 1)     CartesianIndex(1020, 1024, 1)\n",
       " CartesianIndex(1021, 1, 1)  …  CartesianIndex(1021, 1024, 1)\n",
       " CartesianIndex(1022, 1, 1)     CartesianIndex(1022, 1024, 1)\n",
       " CartesianIndex(1023, 1, 1)     CartesianIndex(1023, 1024, 1)\n",
       " CartesianIndex(1024, 1, 1)     CartesianIndex(1024, 1024, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rmain = ntuple(i->i == ndims(A) ? Base.OneTo(1) : axes(A)[i], ndims(A)) |> CartesianIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9ee8b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1×16 CartesianIndices{3, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}, Base.OneTo{Int64}}}:\n",
       "[:, :, 1] =\n",
       " CartesianIndex(1, 1, 1)\n",
       "\n",
       "[:, :, 2] =\n",
       " CartesianIndex(1, 1, 2)\n",
       "\n",
       "[:, :, 3] =\n",
       " CartesianIndex(1, 1, 3)\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 14] =\n",
       " CartesianIndex(1, 1, 14)\n",
       "\n",
       "[:, :, 15] =\n",
       " CartesianIndex(1, 1, 15)\n",
       "\n",
       "[:, :, 16] =\n",
       " CartesianIndex(1, 1, 16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rbatch = ntuple(i->i != ndims(A) ? Base.OneTo(1) : axes(A)[i], ndims(A)) |> CartesianIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "112a6f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.40783522\n",
       " 0.40840903\n",
       " 0.40835467\n",
       " 0.40790036\n",
       " 0.40784442\n",
       " 0.40842506\n",
       " 0.40812257\n",
       " 0.40816554\n",
       " 0.40855056\n",
       " 0.4078624\n",
       " 0.40807194\n",
       " 0.40838\n",
       " 0.40803307\n",
       " 0.40801063\n",
       " 0.40788803\n",
       " 0.40851396"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rmse_kernel(C, A, B, Rmain, Rbatch)\n",
    "    batch = blockIdx().x\n",
    "    Ibatch = Rbatch[batch]\n",
    "    \n",
    "    # initialize the memory\n",
    "    if threadIdx().x == 1\n",
    "        C[batch] = 0\n",
    "    end\n",
    "    sync_threads()\n",
    "    \n",
    "    # grid-stride loop to process each batch in a block\n",
    "    for i in threadIdx().x:blockDim().x:length(Rmain)\n",
    "        Imain = Rmain[i]\n",
    "        I = max(Imain, Ibatch)\n",
    "        a = A[I]\n",
    "        b = B[I]\n",
    "        CUDA.@atomic C[batch] += (a-b)^2\n",
    "    end    \n",
    "    sync_threads()\n",
    "    \n",
    "    # finalize the computation\n",
    "    if threadIdx().x == 1\n",
    "        C[batch] = sqrt(C[batch] / length(Rmain))\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda threads=256 blocks=N rmse_kernel(C, A, B, Rmain, Rbatch)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b022011",
   "metadata": {},
   "source": [
    "We now have a fully general kernel that handles arbitrarily-sized inputs, treating the last dimension as the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff08ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
